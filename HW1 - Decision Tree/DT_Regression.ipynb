{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \"\"\"A Decision Node asks a question. This holds a reference to the question, and to the two child nodes.\"\"\"\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"A Leaf node for regression. This holds predictions based on average, median, and fitting.\"\"\"\n",
    "    def __init__(self, rows):\n",
    "        # Extracting the target values\n",
    "        target_values = [row[-1] for row in rows]\n",
    "        self.predictions = {\n",
    "            'average': np.mean(target_values),\n",
    "            'median': np.median(target_values)\n",
    "        }\n",
    "        # Fitting part using Linear Regression\n",
    "        if len(rows[0]) > 1:  # Check if there are features along with the target variable\n",
    "            features = np.array([row[:-1] for row in rows])\n",
    "            target = np.array(target_values)\n",
    "            model = LinearRegression()\n",
    "            model.fit(features, target)\n",
    "            self.predictions['fitting'] = model\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=float('inf'), min_samples_split=2):\n",
    "        self.tree = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = [X[i] + [y[i]] for i in range(len(X))]\n",
    "        self.tree = self.build_tree(data)\n",
    "\n",
    "    def predict(self, rows, method='average'):\n",
    "        predictions = [self.classify(row, self.tree, method) for row in rows]\n",
    "        return predictions\n",
    "\n",
    "    def build_tree(self, rows, depth=0):\n",
    "        \"\"\"Builds the tree with pre-pruning.\"\"\"\n",
    "        if len(rows) < self.min_samples_split or depth >= self.max_depth:\n",
    "            return Leaf(rows)\n",
    "\n",
    "        gain, question = self.find_best_split(rows)\n",
    "        if gain == 0:\n",
    "            return Leaf(rows)\n",
    "\n",
    "        true_rows, false_rows = self.partition(rows, question)\n",
    "        true_branch = self.build_tree(true_rows, depth + 1)\n",
    "        false_branch = self.build_tree(false_rows, depth + 1)\n",
    "\n",
    "        return DecisionNode(question, true_branch, false_branch)\n",
    "\n",
    "\n",
    "    def find_best_split(self, rows):\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        current_variance = self.variance(rows)\n",
    "        n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "        for col in range(n_features):  # for each feature\n",
    "            values = set([row[col] for row in rows])  # unique values in the column\n",
    "            for val in values:  # for each value\n",
    "                question = (col, val)\n",
    "                true_rows, false_rows = self.partition(rows, question)\n",
    "\n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.info_gain(true_rows, false_rows, current_variance)\n",
    "\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_question = gain, question\n",
    "\n",
    "        return best_gain, best_question\n",
    "\n",
    "    def variance(self, rows):\n",
    "        \"\"\"Calculates the variance of target values in a dataset.\"\"\"\n",
    "        if not rows:\n",
    "            return 0\n",
    "        targets = [row[-1] for row in rows]\n",
    "        mean_value = np.mean(targets)\n",
    "        variance = sum((x - mean_value) ** 2 for x in targets) / len(targets)\n",
    "        return variance\n",
    "\n",
    "    def info_gain(self, left, right, current_variance):\n",
    "        \"\"\"Information Gain based on variance reduction.\"\"\"\n",
    "        p = float(len(left)) / (len(left) + len(right))\n",
    "        return current_variance - p * self.variance(left) - (1 - p) * self.variance(right)\n",
    "\n",
    "    def classify(self, row, node, method):\n",
    "        \"\"\"Classify a new data point based on the tree.\"\"\"\n",
    "        if isinstance(node, Leaf):\n",
    "            if method == 'fitting' and 'fitting' in node.predictions:\n",
    "                return node.predictions['fitting'].predict([row[:-1]])[0]\n",
    "            else:\n",
    "                return node.predictions[method]\n",
    "\n",
    "        if row[node.question[0]] >= node.question[1]:\n",
    "            return self.classify(row, node.true_branch, method)\n",
    "        else:\n",
    "            return self.classify(row, node.false_branch, method)\n",
    "\n",
    "    def partition(self, rows, question):\n",
    "        \"\"\"Partitions a dataset.\"\"\"\n",
    "        true_rows, false_rows = [], []\n",
    "        col, val = question\n",
    "        for row in rows:\n",
    "            if row[col] >= val:\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(filename):\n",
    "        dataset = []\n",
    "        with open(filename, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                dataset.append(row)\n",
    "        dataset.pop(0)\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def split_dataset_train_test(dataset, split_ratio):\n",
    "        random.shuffle(dataset)\n",
    "        split_index = int(len(dataset) * split_ratio)\n",
    "        train_data = dataset[:split_index]\n",
    "        test_data = dataset[split_index:]\n",
    "        return train_data, test_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_tree(node, spacing=\"\"):\n",
    "        if isinstance(node, Leaf):\n",
    "            print(spacing + \"Predict\", node.predictions)\n",
    "            return\n",
    "\n",
    "        print(spacing + str(node.question))\n",
    "        \n",
    "        print(spacing + '--> True:')\n",
    "        DecisionTreeRegressor.print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "        print(spacing + '--> False:')\n",
    "        DecisionTreeRegressor.print_tree(node.false_branch, spacing + \"  \")\n",
    "\n",
    "    def accuracy_on_combined_data(self, rows):\n",
    "        correct = 0\n",
    "        for row in rows:\n",
    "            if self.classify(row[:-1], self.tree) == row[-1]:\n",
    "                correct += 1\n",
    "        return correct / len(rows) if rows else 0\n",
    "    \n",
    "    def prune(self, node, validation_rows):\n",
    "        if isinstance(node, Leaf):\n",
    "            return node\n",
    "\n",
    "        # Prune true and false branches first\n",
    "        node.true_branch = self.prune(node.true_branch, validation_rows)\n",
    "        node.false_branch = self.prune(node.false_branch, validation_rows)\n",
    "\n",
    "        # If both branches are now leaves, consider pruning (merging) them\n",
    "        if isinstance(node.true_branch, Leaf) and isinstance(node.false_branch, Leaf):\n",
    "            # Evaluate accuracy with this node\n",
    "            accuracy_before_pruning = self.accuracy_on_combined_data(validation_rows)\n",
    "\n",
    "            # Create a leaf node that replaces this DecisionNode\n",
    "            merged_leaf = Leaf(validation_rows)\n",
    "\n",
    "            # Temporarily replace this DecisionNode with merged_leaf\n",
    "            original_node = node\n",
    "            self.tree = merged_leaf\n",
    "\n",
    "            # Evaluate accuracy without this node (i.e., with it replaced by merged_leaf)\n",
    "            accuracy_after_pruning = self.accuracy_on_combined_data(validation_rows)\n",
    "\n",
    "            # Restore the original tree structure if pruning did not improve accuracy\n",
    "            if accuracy_after_pruning < accuracy_before_pruning:\n",
    "                self.tree = original_node\n",
    "\n",
    "            return merged_leaf if accuracy_after_pruning >= accuracy_before_pruning else original_node\n",
    "\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (handling missing values and normalize features, ...)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('housing.csv')\n",
    "\n",
    "# Handling missing values (example: using median to fill missing values)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_num = df.drop('ocean_proximity', axis=1)  # Assuming 'ocean_proximity' is categorical\n",
    "imputed_df = imputer.fit_transform(df_num)\n",
    "df_num = pd.DataFrame(imputed_df, columns=df_num.columns)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = scaler.fit_transform(df_num)\n",
    "df_num_scaled = pd.DataFrame(df_num_scaled, columns=df_num.columns)\n",
    "\n",
    "# Split dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_num_scaled.drop('median_house_value', axis=1), df_num_scaled['median_house_value'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to lists for compatibility with our DecisionTreeClassifier\n",
    "train_data = [list(x) + [y] for x, y in zip(X_train.values.tolist(), y_train.tolist())]\n",
    "test_data = [list(x) + [y] for x, y in zip(X_test.values.tolist(), y_test.tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the code on data\n",
    "regressor = DecisionTreeRegressor(max_depth=5, min_samples_split=10)\n",
    "regressor.fit(X_train.values.tolist(), y_train.tolist())\n",
    "\n",
    "predictions = regressor.predict(test_data, method=\"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.4594402174223271\n",
      "Mean Squared Error (MSE): 0.4226878346002094\n",
      "Root Mean Squared Error (RMSE): 0.6501444720984786\n",
      "R-squared (R²): 0.570492783284815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
